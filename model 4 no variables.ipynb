{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from text_preprocessing import *\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "df shape:  (3000, 9)\n",
      "\n",
      "rows with null values:  Int64Index([1551, 1552], dtype='int64')\n",
      "rows that arent twitter:  1\n",
      "\n",
      "dropped one row, fixed other null by dropping platform col, as unneeded\n",
      "\n",
      "type of friends col:  int32\n",
      "\n",
      " country breakdown:  GB    2991\n",
      "GG       5\n",
      "JE       2\n",
      "IM       1\n",
      "Name: location.country, dtype: int64\n",
      "\n",
      "df shape:  (2999, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Projects\\bitbucket\\fancy-a-challenge-william\\utils.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  combined_df = pd.concat([csv_df,txt_df])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author.properties.friends</th>\n",
       "      <th>author.properties.status_count</th>\n",
       "      <th>author.properties.verified</th>\n",
       "      <th>content.body</th>\n",
       "      <th>location.country</th>\n",
       "      <th>location.latitude</th>\n",
       "      <th>location.longitude</th>\n",
       "      <th>properties.sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1689</td>\n",
       "      <td>22566.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Can't believe I'm missing Love Island ðŸ˜©</td>\n",
       "      <td>GB</td>\n",
       "      <td>51.570448</td>\n",
       "      <td>0.457135</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Last tweet about future wedding..... if I actu...</td>\n",
       "      <td>GB</td>\n",
       "      <td>52.969744</td>\n",
       "      <td>-1.172266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>568</td>\n",
       "      <td>8375.0</td>\n",
       "      <td>False</td>\n",
       "      <td>How many times does he wonna say the phrase \"i...</td>\n",
       "      <td>GB</td>\n",
       "      <td>51.394369</td>\n",
       "      <td>0.026299</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1845</td>\n",
       "      <td>19394.0</td>\n",
       "      <td>False</td>\n",
       "      <td>...even better if time travel were invented an...</td>\n",
       "      <td>GB</td>\n",
       "      <td>51.648544</td>\n",
       "      <td>-3.804292</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1617</td>\n",
       "      <td>17922.0</td>\n",
       "      <td>False</td>\n",
       "      <td>@GreenAlty @ShakeyStephens My Mum in mid 80s p...</td>\n",
       "      <td>GB</td>\n",
       "      <td>53.543471</td>\n",
       "      <td>-2.653238</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author.properties.friends  author.properties.status_count  \\\n",
       "0                       1689                         22566.0   \n",
       "1                        114                          1377.0   \n",
       "2                        568                          8375.0   \n",
       "3                       1845                         19394.0   \n",
       "4                       1617                         17922.0   \n",
       "\n",
       "  author.properties.verified  \\\n",
       "0                      False   \n",
       "1                      False   \n",
       "2                      False   \n",
       "3                      False   \n",
       "4                      False   \n",
       "\n",
       "                                        content.body location.country  \\\n",
       "0            Can't believe I'm missing Love Island ðŸ˜©               GB   \n",
       "1  Last tweet about future wedding..... if I actu...               GB   \n",
       "2  How many times does he wonna say the phrase \"i...               GB   \n",
       "3  ...even better if time travel were invented an...               GB   \n",
       "4  @GreenAlty @ShakeyStephens My Mum in mid 80s p...               GB   \n",
       "\n",
       "   location.latitude  location.longitude  properties.sentiment  \n",
       "0          51.570448            0.457135                   1.0  \n",
       "1          52.969744           -1.172266                   1.0  \n",
       "2          51.394369            0.026299                  -1.0  \n",
       "3          51.648544           -3.804292                  -1.0  \n",
       "4          53.543471           -2.653238                   0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = preprocess_values(load_combined_df())\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['sentiment'] = combined_df['properties.sentiment'].apply(lambda x: 'neg' if x ==-1 else 'neut' if x==0 else \"pos\")\n",
    "combined_df['sentiment'].dtype\n",
    "combined_df[['properties.sentiment','sentiment']].head()\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "combined_df.shape\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "\n",
    "X_ROS, y_ROS = ros.fit_sample(combined_df, combined_df['sentiment'])\n",
    "ros_df = pd.DataFrame(X_ROS,columns=combined_df.columns)\n",
    "ros_df['sentiment'].value_counts()\n",
    "\n",
    "data_train,data_test = train_test_split(ros_df,test_size=0.3,random_state=37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3285    det var vrre n s killarna som snacka spanska v...\n",
       "424     is  smirking face microphone smiling face with...\n",
       "168      folded hands folded hands folded hands folded...\n",
       "757                                             at with  \n",
       "3753                       blimey exclamation exclamation\n",
       "2004     try to get tickets  grinning face with big ey...\n",
       "3161     i feel you face with tears of joy i could do ...\n",
       "1622    alfie deyes voice is the worst sound on this e...\n",
       "827                                           never been \n",
       "1402     adorable pup xx id maybe have you beg more or...\n",
       "3174     mee just please red heart person raising hand...\n",
       "218     found this great venue 'the happy chef chinese...\n",
       "1683                   yes but i do love sam smith though\n",
       "858            is not gna stress it anymore sleeping face\n",
       "1541                      cannot wait face blowing a kiss\n",
       "3667    a long time by mayer hawthorne is in harrods l...\n",
       "2898     no worries kidda still enjoying it like smili...\n",
       "1286                        i freaking love steven adams \n",
       "4073                                     woof exclamation\n",
       "3503                                            of course\n",
       "3843    it is been an exceedingly long day of very mix...\n",
       "2       how many times does he wonna say the phrase i ...\n",
       "3747     se mide normalmente tasa de homicidios y deli...\n",
       "1055    j'suis archi fine en vrai c'est mtn je crame o...\n",
       "3705          i cannot wait to read books all summer long\n",
       "1217    i am blessed with my layout thank you  for bei...\n",
       "4080    is it hard being literal angels how are you so...\n",
       "1356    so proud of my queen  for her hard work tonigh...\n",
       "1549                                           i am crazy\n",
       "2179         my shoe and accessory wish list is now on   \n",
       "                              ...                        \n",
       "234     different gear of an evening with the  and  on...\n",
       "3982    alfie deyes voice is the worst sound on this e...\n",
       "1594      happy birthday bro smiling face with sunglasses\n",
       "2499                         i did it first hate me more \n",
       "3417       face with tears of joy face with tears of joy \n",
       "1506                i know quite the young ruffian thomas\n",
       "3427                                           two hearts\n",
       "819                                            sup wi you\n",
       "3970     might as well make you a bday cake covered in...\n",
       "3825                               i love my london life \n",
       "244     average wind n a mph force n a barometer oneze...\n",
       "1043                     just posted a photo basingstoke \n",
       "1880                                            xbox one \n",
       "2650                                                     \n",
       "2700    wallah i can hear someone talking and i am ful...\n",
       "984      a little but i have to say seems they are not...\n",
       "387     your music has forever provided me with the se...\n",
       "1512    i am so excited to get my nails done after exa...\n",
       "3848        they do and it was great i could count the...\n",
       "4158                    just what the tour coaches needed\n",
       "2015            'fleur de lis' watch ya mouth exclamation\n",
       "2775                                   loudly crying face\n",
       "2392     do not quite agree do not think it is brave t...\n",
       "575     watch nates new video it is the bomb face with...\n",
       "3626    tomorrow is my friday yet it is thursday disgu...\n",
       "3706     me and my friends still occasionally blast it...\n",
       "4118                        ah finally a wank exclamation\n",
       "988                                  nicole question lol \n",
       "844      power we have had a selfie drought lately any...\n",
       "1935    lol had a great nap unamused face nasib baik k...\n",
       "Name: content.body, Length: 2946, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Dense, Input, Flatten,GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional,SpatialDropout1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = string.decode('utf-8')\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = data_train['content.body'].apply(lambda x: preprocess_no_variables(x))\n",
    "#labels = data_train['sentiment']\n",
    "labels = data_train['properties.sentiment']\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('glove.6B.300d.txt'):\n",
    "    if not os.path.exists('glove.6B.zip'):\n",
    "        ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    if not os.path.exists('glove.6B.300d.txt'):\n",
    "        ! unzip glove.6B.zip\n",
    "        !rm glove.6B.100d.txt\n",
    "        !rm glove.6B.50d.txt\n",
    "        !rm glove.6B.200d.txt\n",
    "    \n",
    "import codecs\n",
    "EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
    "EMBEDDING_DIM = int(EMBEDDING_FILE.split('.')[-2][:-1])\n",
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.abspath('glove.6B.300d.txt'),encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wburke\\projects\\venv36\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train['sent'] = data_train['properties.sentiment'].apply(lambda x: x + 1).astype('str')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "temp = np.array(data_train['sent']).reshape(-1, 1)\n",
    "x = onehotencoder.fit_transform(temp).toarray()\n",
    "labels = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neut</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neg  neut  pos\n",
       "0  0.0   1.0  0.0\n",
       "1  0.0   1.0  0.0\n",
       "2  0.0   1.0  0.0\n",
       "3  0.0   1.0  0.0\n",
       "4  0.0   0.0  1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_columns=['neg','neut','pos']\n",
    "y_train = pd.DataFrame(x,columns=sent_columns)\n",
    "assert(y_train[y_train.isnull().any(axis=1)].empty)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: 2946\n",
      "Shape of label tensor: 2946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wburke\\projects\\venv36\\lib\\site-packages\\keras_preprocessing\\text.py:177: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1263, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Shape of data tensor:', len(texts))\n",
    "print('Shape of label tensor:', len(labels))\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(data_test.shape)\n",
    "\n",
    "test_texts= data_test['content.body'].apply(lambda x: preprocess_no_variables(x)).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0  738    3]\n",
      " [   0    0    0 ...    2 1243   63]\n",
      " [   0    0    0 ...   10   15   13]\n",
      " ...\n",
      " [   0    0    0 ... 3158   12  655]\n",
      " [   0    0    0 ...    0    0    0]\n",
      " [   0    0    0 ...  128  624 3159]]\n"
     ]
    }
   ],
   "source": [
    "test_tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "test_tokenizer.fit_on_texts(test_texts)\n",
    "test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "test_word_index = test_tokenizer.word_index\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(test_data)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embedding_matrix: 4947\n",
      "Traing and validation set number of positive and negative reviews\n",
      "[887. 881. 884.]\n",
      "[ 90. 104. 100.]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print ('Length of embedding_matrix:', embedding_matrix.shape[0])\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            mask_zero=False,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "\n",
    "print('Traing and validation set number of positive and negative reviews')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     1484100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 200, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 128)     186880      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200, 128)     74112       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          328         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          98560       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,844,751\n",
      "Trainable params: 1,844,751\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "sentLayer = SpatialDropout1D(0.1)(embedded_sequences)\n",
    "l_lstm = Bidirectional(LSTM(64, return_sequences=True))(sentLayer)\n",
    "x = Bidirectional(GRU(64, return_sequences=True))(l_lstm)\n",
    "\n",
    "x1 = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "x2 = GlobalAveragePooling1D()(x)\n",
    "x3 = GlobalMaxPooling1D()(x)\n",
    "x = Concatenate()([x1, x2, x3])\n",
    "\n",
    "dense_1 = Dense(256,activation='relu')(x)\n",
    "dropout_2=Dropout(0.15)(dense_1)\n",
    "dense_2 = Dense(3, activation='softmax')(dropout_2)\n",
    "\n",
    "model = Model(sequence_input, dense_2)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wburke\\projects\\venv36\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2652 samples, validate on 294 samples\n",
      "Epoch 1/10\n",
      "2652/2652 [==============================] - 74s 28ms/step - loss: 1.0741 - acc: 0.4657 - val_loss: 0.8348 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63946, saving model to predict-goal-weights.best.hdf5\n",
      "Epoch 2/10\n",
      "2652/2652 [==============================] - 70s 26ms/step - loss: 0.8409 - acc: 0.6346 - val_loss: 0.6946 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63946 to 0.72109, saving model to predict-goal-weights.best.hdf5\n",
      "Epoch 3/10\n",
      "2652/2652 [==============================] - 68s 25ms/step - loss: 0.6735 - acc: 0.7315 - val_loss: 0.6696 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.72109\n",
      "Epoch 4/10\n",
      "2652/2652 [==============================] - 70s 27ms/step - loss: 0.5264 - acc: 0.7998 - val_loss: 0.6458 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.72109 to 0.75170, saving model to predict-goal-weights.best.hdf5\n",
      "Epoch 5/10\n",
      "2652/2652 [==============================] - 68s 26ms/step - loss: 0.4212 - acc: 0.8450 - val_loss: 0.6819 - val_acc: 0.7245\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.75170\n",
      "Epoch 6/10\n",
      "2652/2652 [==============================] - 66s 25ms/step - loss: 0.3175 - acc: 0.8929 - val_loss: 0.6619 - val_acc: 0.7857\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.75170 to 0.78571, saving model to predict-goal-weights.best.hdf5\n",
      "Epoch 7/10\n",
      " 736/2652 [=======>......................] - ETA: 46s - loss: 0.2507 - acc: 0.9144"
     ]
    }
   ],
   "source": [
    "filepath=\"predict-goal-weights.best.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "callbacks_list = [checkpoint,early_stop]\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=32,shuffle=True,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open('output.csv', 'w') \n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['statement','label'])\n",
    "\n",
    "a=np.array(model.predict(test_data, batch_size=32, verbose=1))\n",
    "\n",
    "label_names = ['negative','neutral','positive']\n",
    "\n",
    "output=[]\n",
    "for i in range(len(test_texts)):\n",
    "    text = test_texts[i]\n",
    "    label_col = label_names[np.argmax(a,axis=1)[i]]\n",
    "    output_a=[text,label_col]\n",
    "    output.append(output_a)\n",
    "    print(label_col,':',text)\n",
    "print(output)\n",
    "model.save('Twitter-LSTM-Model.h5')\n",
    " \n",
    "writer.writerows(output)\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
